{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BogdanBozga/Ai_TensorFlow_Lite_MiniLego/blob/main/TFLite2_Object_Detction_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxb8_h-QFErO"
      },
      "source": [
        "#1.&nbsp;Install TensorFlow Object Detection Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypWGYdPlLRUN",
        "outputId": "0c04f301-2078-4ce0-d4aa-550387ad27f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 3657, done.\u001b[K\n",
            "remote: Counting objects: 100% (3657/3657), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3054/3054), done.\u001b[K\n",
            "remote: Total 3657 (delta 968), reused 1507 (delta 551), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3657/3657), 47.39 MiB | 10.78 MiB/s, done.\n",
            "Resolving deltas: 100% (968/968), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone the tensorflow models repository from GitHub\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6QPmVBSlLTzM"
      },
      "outputs": [],
      "source": [
        "# Copy setup files into models/research folder\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "#cp object_detection/packages/tf2/setup.py ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NRBnuCKjM4Bd"
      },
      "outputs": [],
      "source": [
        "# Modify setup.py file to install the tf-models-official repository targeted at TF v2.8.0\n",
        "import re\n",
        "with open('/content/models/research/object_detection/packages/tf2/setup.py') as f:\n",
        "    s = f.read()\n",
        "\n",
        "with open('/content/models/research/setup.py', 'w') as f:\n",
        "    # Set fine_tune_checkpoint path\n",
        "    s = re.sub('tf-models-official>=2.5.1',\n",
        "               'tf-models-official==2.11.0', s)\n",
        "    f.write(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OLDnCkLLwLr6",
        "outputId": "a3c450a8-47f0-45ce-b41d-308068a79a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./models/research\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.44.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (4.9.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (0.29.33)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Collecting tf-slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 KB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (2.0.6)\n",
            "Collecting lvis\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (1.7.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (1.3.5)\n",
            "Collecting tf-models-official==2.11.0\n",
            "  Downloading tf_models_official-2.11.0-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow_io\n",
            "  Downloading tensorflow_io-0.29.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (2.9.0)\n",
            "Collecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacrebleu<=2.2.0\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.6/116.6 KB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py-cpuinfo>=3.3.0\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting tensorflow-text~=2.11.0\n",
            "  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.8/dist-packages (from tf-models-official==2.11.0->object-detection==0.1) (0.5.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.8/dist-packages (from tf-models-official==2.11.0->object-detection==0.1) (1.5.12)\n",
            "Collecting pyyaml<6.0,>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m662.4/662.4 KB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python-headless==4.5.2.52\n",
            "  Downloading opencv_python_headless-4.5.2.52-cp38-cp38-manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tf-models-official==2.11.0->object-detection==0.1) (1.21.6)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.8/dist-packages (from tf-models-official==2.11.0->object-detection==0.1) (2.70.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tf-models-official==2.11.0->object-detection==0.1) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.8/dist-packages (from tf-models-official==2.11.0->object-detection==0.1) (4.8.1)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-models-official==2.11.0->object-detection==0.1) (5.4.8)\n",
            "Collecting tensorflow~=2.11.0\n",
            "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting immutabledict\n",
            "  Downloading immutabledict-2.2.3-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.8/dist-packages (from tf-models-official==2.11.0->object-detection==0.1) (4.1.3)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->object-detection==0.1) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.8.10)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2022.6.2)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from tf-slim->object-detection==0.1) (1.3.0)\n",
            "Collecting objsize<0.7.0,>=0.6.1\n",
            "  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.51.1)\n",
            "Collecting fasteners<1.0,>=0.3\n",
            "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
            "Collecting fastavro<2,>=0.23.6\n",
            "  Downloading fastavro-1.7.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httplib2<0.21.0,>=0.8 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (0.17.4)\n",
            "Requirement already satisfied: pyarrow<10.0.0,>=0.15.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (9.0.0)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.22.2)\n",
            "Collecting zstandard<1,>=0.18.0\n",
            "  Downloading zstandard-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.2/526.2 KB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle~=2.2.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (2.2.0)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 KB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Requirement already satisfied: protobuf<4,>3.12.2 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (3.19.6)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.8.5-cp38-cp38-manylinux_2_28_x86_64.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (4.4.0)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from lvis->object-detection==0.1) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from lvis->object-detection==0.1) (0.11.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.8/dist-packages (from lvis->object-detection==0.1) (4.6.0.66)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.29.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_io->object-detection==0.1) (0.29.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.11.0->object-detection==0.1) (0.1.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.11.0->object-detection==0.1) (2.16.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.11.0->object-detection==0.1) (2.11.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.11.0->object-detection==0.1) (4.1.1)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official==2.11.0->object-detection==0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official==2.11.0->object-detection==0.1) (2022.12.7)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official==2.11.0->object-detection==0.1) (7.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official==2.11.0->object-detection==0.1) (4.64.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (4.0.0)\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (2.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (1.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (57.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (15.0.6.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (0.2.0)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-23.1.4-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (21.3)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.11.0->object-detection==0.1) (0.1.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.8/dist-packages (from oauth2client->tf-models-official==2.11.0->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from oauth2client->tf-models-official==2.11.0->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from oauth2client->tf-models-official==2.11.0->object-detection==0.1) (4.9)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval->tf-models-official==2.11.0->object-detection==0.1) (1.0.2)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons->tf-models-official==2.11.0->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official==2.11.0->object-detection==0.1) (5.10.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official==2.11.0->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official==2.11.0->object-detection==0.1) (1.12.0)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official==2.11.0->object-detection==0.1) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official==2.11.0->object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official==2.11.0->object-detection==0.1) (0.10.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (0.38.4)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.8/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tf-models-official==2.11.0->object-detection==0.1) (3.11.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.11.0->object-detection==0.1) (1.58.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.11.0->object-detection==0.1) (5.2.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.11.0->object-detection==0.1) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.11.0->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (1.8.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.11.0->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (6.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official==2.11.0->object-detection==0.1) (3.2.2)\n",
            "Building wheels for collected packages: object-detection, avro-python3, dill, seqeval, docopt\n",
            "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=21919979 sha256=ea8f827aaf8f9622dbdd5d785558023709a65cda7a9b9848aa8240bd507232d0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zohtvb83/wheels/7d/96/c1/072a751379735e8dfdada1def1c62a89afb3cc45654fd6fd28\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=44009 sha256=f6b0313c4fd26b9a1b8db717f708893c91fac8b2c970037e77ceed81f4d4fc09\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/73/e9/d273421f5723c4bf544dcf9eb097bda94421ef8d3252699f0a\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=195d9df6b49d438b735a5ee0014d32babbfeaccd094880891615897cb9bcb7dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/35/78/e9004fa30578734db7f10e7a211605f3f0778d2bdde38a239d\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=9e31e61ed9a813c9c68b11b281c20734350651e4a7e64311c8be20d29f793cc1\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=07653cc22c666190b3388c8d5a1897e3d053ed1edfe3f952cdaf4284ad6c6d08\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "Successfully built object-detection avro-python3 dill seqeval docopt\n",
            "Installing collected packages: sentencepiece, py-cpuinfo, flatbuffers, docopt, zstandard, tf-slim, tensorflow-model-optimization, tensorflow_io, tensorflow-estimator, pyyaml, pyparsing, pymongo, portalocker, orjson, opencv-python-headless, objsize, keras, immutabledict, fasteners, fastavro, dill, colorama, avro-python3, sacrebleu, hdfs, tensorflow-addons, seqeval, lvis, apache-beam, tensorboard, tensorflow, tensorflow-text, tf-models-official, object-detection\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.3.3\n",
            "    Uninstalling pymongo-4.3.3:\n",
            "      Successfully uninstalled pymongo-4.3.3\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.7.0.68\n",
            "    Uninstalling opencv-python-headless-4.7.0.68:\n",
            "      Successfully uninstalled opencv-python-headless-4.7.0.68\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "Successfully installed apache-beam-2.44.0 avro-python3-1.10.2 colorama-0.4.6 dill-0.3.1.1 docopt-0.6.2 fastavro-1.7.0 fasteners-0.18 flatbuffers-23.1.4 hdfs-2.7.0 immutabledict-2.2.3 keras-2.11.0 lvis-0.5.3 object-detection-0.1 objsize-0.6.1 opencv-python-headless-4.5.2.52 orjson-3.8.5 portalocker-2.7.0 py-cpuinfo-9.0.0 pymongo-3.13.0 pyparsing-2.4.7 pyyaml-5.4.1 sacrebleu-2.2.0 sentencepiece-0.1.97 seqeval-1.2.2 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-addons-0.19.0 tensorflow-estimator-2.11.0 tensorflow-model-optimization-0.7.3 tensorflow-text-2.11.0 tensorflow_io-0.29.0 tf-models-official-2.11.0 tf-slim-1.1.0 zstandard-0.19.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.29.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.1.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.25.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (2.4.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "# Install the Object Detection API\n",
        "!pip install /content/models/research/\n",
        "\n",
        "# Need to downgrade to TF v2.8.0 due to Colab compatibility bug with TF v2.10 (as of 10/03/22)\n",
        "!pip install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh_HPMOqWH9z",
        "outputId": "c062e551-35d1-446c-de8f-8739eca8d7e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-19 19:19:40.878668: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-01-19 19:19:41.937768: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-01-19 19:19:41.937905: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-01-19 19:19:41.937929: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Running tests under Python 3.8.10: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "2023-01-19 19:19:46.639264: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "W0119 19:19:46.905659 139651249134656 model_builder.py:1112] Building experimental DeepMAC meta-arch. Some features may be omitted.\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.48s\n",
            "I0119 19:19:47.195653 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.48s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.66s\n",
            "I0119 19:19:47.855057 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.66s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.28s\n",
            "I0119 19:19:48.137418 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.28s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.25s\n",
            "I0119 19:19:48.389579 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.25s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 2.62s\n",
            "I0119 19:19:51.013517 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 2.62s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "I0119 19:19:51.019903 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.04s\n",
            "I0119 19:19:51.056349 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.04s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n",
            "I0119 19:19:51.081018 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.03s\n",
            "I0119 19:19:51.108088 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.03s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.48s\n",
            "I0119 19:19:51.591122 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.48s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.15s\n",
            "I0119 19:19:51.746131 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.15s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.18s\n",
            "I0119 19:19:51.930932 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.18s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.17s\n",
            "I0119 19:19:52.098928 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.17s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.16s\n",
            "I0119 19:19:52.257447 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.16s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.07s\n",
            "I0119 19:19:52.328992 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.07s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "I0119 19:19:52.825438 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I0119 19:19:52.825682 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 64\n",
            "I0119 19:19:52.825804 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 3\n",
            "I0119 19:19:52.829830 139651249134656 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0119 19:19:52.876674 139651249134656 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0119 19:19:52.876864 139651249134656 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0119 19:19:52.993084 139651249134656 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0119 19:19:52.993294 139651249134656 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0119 19:19:53.314191 139651249134656 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0119 19:19:53.314387 139651249134656 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0119 19:19:53.681585 139651249134656 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0119 19:19:53.681807 139651249134656 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0119 19:19:54.067636 139651249134656 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0119 19:19:54.067835 139651249134656 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0119 19:19:54.538680 139651249134656 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0119 19:19:54.538903 139651249134656 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0119 19:19:55.142846 139651249134656 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0119 19:19:55.143049 139651249134656 efficientnet_model.py:143] round_filter input=320 output=320\n",
            "I0119 19:19:55.399151 139651249134656 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
            "I0119 19:19:55.468428 139651249134656 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0119 19:19:55.582126 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
            "I0119 19:19:55.582315 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 88\n",
            "I0119 19:19:55.582397 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 4\n",
            "I0119 19:19:55.584868 139651249134656 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0119 19:19:55.612834 139651249134656 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0119 19:19:55.612978 139651249134656 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0119 19:19:55.880564 139651249134656 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0119 19:19:55.880778 139651249134656 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0119 19:19:56.389545 139651249134656 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0119 19:19:56.389762 139651249134656 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0119 19:19:56.907390 139651249134656 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0119 19:19:56.907582 139651249134656 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0119 19:19:57.578266 139651249134656 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0119 19:19:57.578469 139651249134656 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0119 19:19:58.281652 139651249134656 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0119 19:19:58.281869 139651249134656 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0119 19:19:59.111117 139651249134656 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0119 19:19:59.111323 139651249134656 efficientnet_model.py:143] round_filter input=320 output=320\n",
            "I0119 19:19:59.449339 139651249134656 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
            "I0119 19:19:59.512563 139651249134656 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0119 19:19:59.606712 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
            "I0119 19:19:59.606908 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 112\n",
            "I0119 19:19:59.607003 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 5\n",
            "I0119 19:19:59.609710 139651249134656 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0119 19:19:59.634443 139651249134656 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0119 19:19:59.634588 139651249134656 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0119 19:19:59.852467 139651249134656 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0119 19:19:59.852667 139651249134656 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0119 19:20:00.293959 139651249134656 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0119 19:20:00.294166 139651249134656 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0119 19:20:01.082759 139651249134656 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0119 19:20:01.082969 139651249134656 efficientnet_model.py:143] round_filter input=80 output=88\n",
            "I0119 19:20:01.783277 139651249134656 efficientnet_model.py:143] round_filter input=80 output=88\n",
            "I0119 19:20:01.783480 139651249134656 efficientnet_model.py:143] round_filter input=112 output=120\n",
            "I0119 19:20:02.481462 139651249134656 efficientnet_model.py:143] round_filter input=112 output=120\n",
            "I0119 19:20:02.481686 139651249134656 efficientnet_model.py:143] round_filter input=192 output=208\n",
            "I0119 19:20:03.282791 139651249134656 efficientnet_model.py:143] round_filter input=192 output=208\n",
            "I0119 19:20:03.283012 139651249134656 efficientnet_model.py:143] round_filter input=320 output=352\n",
            "I0119 19:20:03.631792 139651249134656 efficientnet_model.py:143] round_filter input=1280 output=1408\n",
            "I0119 19:20:03.680000 139651249134656 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0119 19:20:03.877362 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
            "I0119 19:20:03.877528 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 160\n",
            "I0119 19:20:03.877585 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 6\n",
            "I0119 19:20:03.880198 139651249134656 efficientnet_model.py:143] round_filter input=32 output=40\n",
            "I0119 19:20:03.912004 139651249134656 efficientnet_model.py:143] round_filter input=32 output=40\n",
            "I0119 19:20:03.912187 139651249134656 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0119 19:20:04.308401 139651249134656 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0119 19:20:04.308589 139651249134656 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0119 19:20:05.039455 139651249134656 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0119 19:20:05.039670 139651249134656 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0119 19:20:05.429153 139651249134656 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0119 19:20:05.429319 139651249134656 efficientnet_model.py:143] round_filter input=80 output=96\n",
            "I0119 19:20:05.852291 139651249134656 efficientnet_model.py:143] round_filter input=80 output=96\n",
            "I0119 19:20:05.852469 139651249134656 efficientnet_model.py:143] round_filter input=112 output=136\n",
            "I0119 19:20:06.284443 139651249134656 efficientnet_model.py:143] round_filter input=112 output=136\n",
            "I0119 19:20:06.284616 139651249134656 efficientnet_model.py:143] round_filter input=192 output=232\n",
            "I0119 19:20:06.788476 139651249134656 efficientnet_model.py:143] round_filter input=192 output=232\n",
            "I0119 19:20:06.788649 139651249134656 efficientnet_model.py:143] round_filter input=320 output=384\n",
            "I0119 19:20:06.963785 139651249134656 efficientnet_model.py:143] round_filter input=1280 output=1536\n",
            "I0119 19:20:07.000419 139651249134656 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0119 19:20:07.065734 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
            "I0119 19:20:07.065869 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 224\n",
            "I0119 19:20:07.065929 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 7\n",
            "I0119 19:20:07.067498 139651249134656 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0119 19:20:07.085230 139651249134656 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0119 19:20:07.085339 139651249134656 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0119 19:20:07.220497 139651249134656 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0119 19:20:07.220657 139651249134656 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0119 19:20:07.565554 139651249134656 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0119 19:20:07.565738 139651249134656 efficientnet_model.py:143] round_filter input=40 output=56\n",
            "I0119 19:20:07.909426 139651249134656 efficientnet_model.py:143] round_filter input=40 output=56\n",
            "I0119 19:20:07.909626 139651249134656 efficientnet_model.py:143] round_filter input=80 output=112\n",
            "I0119 19:20:08.417928 139651249134656 efficientnet_model.py:143] round_filter input=80 output=112\n",
            "I0119 19:20:08.418088 139651249134656 efficientnet_model.py:143] round_filter input=112 output=160\n",
            "I0119 19:20:08.925423 139651249134656 efficientnet_model.py:143] round_filter input=112 output=160\n",
            "I0119 19:20:08.925584 139651249134656 efficientnet_model.py:143] round_filter input=192 output=272\n",
            "I0119 19:20:09.796769 139651249134656 efficientnet_model.py:143] round_filter input=192 output=272\n",
            "I0119 19:20:09.796931 139651249134656 efficientnet_model.py:143] round_filter input=320 output=448\n",
            "I0119 19:20:09.985730 139651249134656 efficientnet_model.py:143] round_filter input=1280 output=1792\n",
            "I0119 19:20:10.020023 139651249134656 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0119 19:20:10.089921 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
            "I0119 19:20:10.090062 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 288\n",
            "I0119 19:20:10.090139 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 7\n",
            "I0119 19:20:10.091680 139651249134656 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0119 19:20:10.108146 139651249134656 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0119 19:20:10.108246 139651249134656 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0119 19:20:10.305752 139651249134656 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0119 19:20:10.305936 139651249134656 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0119 19:20:10.712540 139651249134656 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0119 19:20:10.712724 139651249134656 efficientnet_model.py:143] round_filter input=40 output=64\n",
            "I0119 19:20:11.132769 139651249134656 efficientnet_model.py:143] round_filter input=40 output=64\n",
            "I0119 19:20:11.132956 139651249134656 efficientnet_model.py:143] round_filter input=80 output=128\n",
            "I0119 19:20:11.726117 139651249134656 efficientnet_model.py:143] round_filter input=80 output=128\n",
            "I0119 19:20:11.726280 139651249134656 efficientnet_model.py:143] round_filter input=112 output=176\n",
            "I0119 19:20:12.319269 139651249134656 efficientnet_model.py:143] round_filter input=112 output=176\n",
            "I0119 19:20:12.319445 139651249134656 efficientnet_model.py:143] round_filter input=192 output=304\n",
            "I0119 19:20:13.064299 139651249134656 efficientnet_model.py:143] round_filter input=192 output=304\n",
            "I0119 19:20:13.064457 139651249134656 efficientnet_model.py:143] round_filter input=320 output=512\n",
            "I0119 19:20:13.341495 139651249134656 efficientnet_model.py:143] round_filter input=1280 output=2048\n",
            "I0119 19:20:13.376889 139651249134656 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0119 19:20:13.452616 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
            "I0119 19:20:13.452748 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 384\n",
            "I0119 19:20:13.452819 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 8\n",
            "I0119 19:20:13.454316 139651249134656 efficientnet_model.py:143] round_filter input=32 output=56\n",
            "I0119 19:20:13.472122 139651249134656 efficientnet_model.py:143] round_filter input=32 output=56\n",
            "I0119 19:20:13.472327 139651249134656 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0119 19:20:13.676791 139651249134656 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0119 19:20:13.676943 139651249134656 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0119 19:20:14.168206 139651249134656 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0119 19:20:14.168367 139651249134656 efficientnet_model.py:143] round_filter input=40 output=72\n",
            "I0119 19:20:14.680022 139651249134656 efficientnet_model.py:143] round_filter input=40 output=72\n",
            "I0119 19:20:14.680187 139651249134656 efficientnet_model.py:143] round_filter input=80 output=144\n",
            "I0119 19:20:15.448539 139651249134656 efficientnet_model.py:143] round_filter input=80 output=144\n",
            "I0119 19:20:15.448741 139651249134656 efficientnet_model.py:143] round_filter input=112 output=200\n",
            "I0119 19:20:16.632643 139651249134656 efficientnet_model.py:143] round_filter input=112 output=200\n",
            "I0119 19:20:16.632828 139651249134656 efficientnet_model.py:143] round_filter input=192 output=344\n",
            "I0119 19:20:17.967929 139651249134656 efficientnet_model.py:143] round_filter input=192 output=344\n",
            "I0119 19:20:17.968129 139651249134656 efficientnet_model.py:143] round_filter input=320 output=576\n",
            "I0119 19:20:18.349138 139651249134656 efficientnet_model.py:143] round_filter input=1280 output=2304\n",
            "I0119 19:20:18.399564 139651249134656 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0119 19:20:18.544161 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
            "I0119 19:20:18.544351 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 384\n",
            "I0119 19:20:18.544441 139651249134656 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 8\n",
            "I0119 19:20:18.546915 139651249134656 efficientnet_model.py:143] round_filter input=32 output=64\n",
            "I0119 19:20:18.575023 139651249134656 efficientnet_model.py:143] round_filter input=32 output=64\n",
            "I0119 19:20:18.575190 139651249134656 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0119 19:20:18.978991 139651249134656 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0119 19:20:18.979183 139651249134656 efficientnet_model.py:143] round_filter input=24 output=48\n",
            "I0119 19:20:19.828975 139651249134656 efficientnet_model.py:143] round_filter input=24 output=48\n",
            "I0119 19:20:19.829191 139651249134656 efficientnet_model.py:143] round_filter input=40 output=80\n",
            "I0119 19:20:20.685961 139651249134656 efficientnet_model.py:143] round_filter input=40 output=80\n",
            "I0119 19:20:20.686157 139651249134656 efficientnet_model.py:143] round_filter input=80 output=160\n",
            "I0119 19:20:21.926823 139651249134656 efficientnet_model.py:143] round_filter input=80 output=160\n",
            "I0119 19:20:21.927035 139651249134656 efficientnet_model.py:143] round_filter input=112 output=224\n",
            "I0119 19:20:23.096764 139651249134656 efficientnet_model.py:143] round_filter input=112 output=224\n",
            "I0119 19:20:23.096961 139651249134656 efficientnet_model.py:143] round_filter input=192 output=384\n",
            "I0119 19:20:24.657186 139651249134656 efficientnet_model.py:143] round_filter input=192 output=384\n",
            "I0119 19:20:24.657386 139651249134656 efficientnet_model.py:143] round_filter input=320 output=640\n",
            "I0119 19:20:25.101152 139651249134656 efficientnet_model.py:143] round_filter input=1280 output=2560\n",
            "I0119 19:20:25.135880 139651249134656 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 33.14s\n",
            "I0119 19:20:25.471397 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 33.14s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "I0119 19:20:25.498167 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "I0119 19:20:25.499771 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "I0119 19:20:25.500222 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "I0119 19:20:25.501497 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTF2Test.test_session\n",
            "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "I0119 19:20:25.502734 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "I0119 19:20:25.503127 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "I0119 19:20:25.504001 139651249134656 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 24 tests in 39.787s\n",
            "\n",
            "OK (skipped=1)\n"
          ]
        }
      ],
      "source": [
        "# Run Model Bulider Test file, just to verify everything's working properly\n",
        "!python /content/models/research/object_detection/builders/model_builder_tf2_test.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eydREUsMGUUR"
      },
      "source": [
        "# 2.&nbsp;Upload Image Dataset and Prepare Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE1MtX4HGQA4"
      },
      "source": [
        "### 2.1 Upload images\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zshEP-26P1O"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5st2VnbH6BHf"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/Colab_Notebooks/images.zip /content\n",
        "!cp /content/drive/MyDrive/Colab_Notebooks/train_val_test_split.py /content\n",
        "!cp /content/drive/MyDrive/Colab_Notebooks/create_csv.py /content\n",
        "!cp /content/drive/MyDrive/Colab_Notebooks/create_tfrecord.py /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHjOhoSGYwT7"
      },
      "source": [
        "## 2.2 Split images into train, validation, and test folders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGvoHH-unSVO"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/images\n",
        "!mkdir /content/images\n",
        "!unzip -q images.zip -d /content/images/all\n",
        "!mkdir /content/images/train; mkdir /content/images/validation; mkdir /content/images/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfuZpmdBLjh-"
      },
      "outputs": [],
      "source": [
        "!python train_val_test_split.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p--K1PJXEgNo"
      },
      "source": [
        "## 2.3 Create Labelmap and TFRecords\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib-92FVqMFnY"
      },
      "outputs": [],
      "source": [
        "# !rm /content/labelmap.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DE_r4MKY7ln"
      },
      "outputs": [],
      "source": [
        "### This creates a a \"labelmap.txt\" file with a list of classes the object detection model will detect.\n",
        "%%bash\n",
        "cat <<EOF >> /content/labelmap.txt\n",
        "lego-car-mirror\n",
        "lego-door-2\n",
        "lego-door-4\n",
        "lego-long-4\n",
        "lego-short-2\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvJZxaARGEaM"
      },
      "outputs": [],
      "source": [
        "!cat /content/labelmap.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pa2VYhTIT1l"
      },
      "source": [
        "Download and run the data conversion scripts from the [GitHub repository](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi) by clicking play on the following three sections of code. They will create TFRecord files for the train and validation datasets, as well as a `labelmap.pbtxt` file which contains the labelmap in a different format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9narBYB3IIXC"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!ls\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tdDbTmHYwu-"
      },
      "outputs": [],
      "source": [
        "# Create CSV data files and TFRecord files\n",
        "!python3 create_csv.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMaKisw6LzNJ"
      },
      "outputs": [],
      "source": [
        "!python3 create_tfrecord.py --csv_input=images/train_labels.csv --labelmap=labelmap.txt --image_dir=images/train --output_path=train.tfrecord\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTyzl2g6Lzf-"
      },
      "outputs": [],
      "source": [
        "!python3 create_tfrecord.py --csv_input=images/validation_labels.csv --labelmap=labelmap.txt --image_dir=images/validation --output_path=val.tfrecord\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNyv_YyDXwMs"
      },
      "source": [
        "We'll store the locations of the TFRecord and labelmap files as variables so we can reference them later in this Colab session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUd2wtfrqedy"
      },
      "outputs": [],
      "source": [
        "train_record_fname = '/content/train.tfrecord'\n",
        "val_record_fname = '/content/val.tfrecord'\n",
        "label_map_pbtxt_fname = '/content/labelmap.pbtxt'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGEUZYAMEZ6f"
      },
      "source": [
        "# 3.&nbsp;Set Up Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN0EUEa3e5Un"
      },
      "outputs": [],
      "source": [
        "# Change the chosen_model variable to deploy different models available in the TF2 object detection zoo\n",
        "chosen_model = 'ssd-mobilenet-v2-fpnlite-320'\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "    'ssd-mobilenet-v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_320x320_coco17_tpu-8',\n",
        "        'base_pipeline_file': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz',\n",
        "    },\n",
        "    'efficientdet-d0': {\n",
        "        'model_name': 'efficientdet_d0_coco17_tpu-32',\n",
        "        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n",
        "    },\n",
        "    'ssd-mobilenet-v2-fpnlite-320': {\n",
        "        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\n",
        "        'base_pipeline_file': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz',\n",
        "    },\n",
        "}\n",
        "\n",
        "model_name = MODELS_CONFIG[chosen_model]['model_name']\n",
        "pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n",
        "base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMG3EEPqPggV"
      },
      "source": [
        "Download the pretrained model file and configuration file by clicking Play on the following section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG4TmJUVrYQ7"
      },
      "outputs": [],
      "source": [
        "# Create \"mymodel\" folder for holding pre-trained weights and configuration files\n",
        "%mkdir /content/models/mymodel/\n",
        "%cd /content/models/mymodel/\n",
        "\n",
        "# Download pre-trained model weights\n",
        "import tarfile\n",
        "download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n",
        "!wget {download_tar}\n",
        "tar = tarfile.open(pretrained_checkpoint)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "# Download training configuration file for model\n",
        "download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n",
        "!wget {download_config}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lYDvJN-n69v"
      },
      "outputs": [],
      "source": [
        "# Set training parameters for the model\n",
        "num_steps = 3000\n",
        "\n",
        "if chosen_model == 'efficientdet-d0':\n",
        "  batch_size = 4\n",
        "else:\n",
        "  batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP0YxVeHuA8E"
      },
      "outputs": [],
      "source": [
        "# pip uninstall tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWopRO2y-4Cf"
      },
      "outputs": [],
      "source": [
        "# pip install tensorflow-object-detection-api\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0Ba6_YNt-8R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_ki9jOqxn7V"
      },
      "outputs": [],
      "source": [
        "# Set file locations and get number of classes for config file\n",
        "pipeline_fname = '/content/models/mymodel/' + base_pipeline_file\n",
        "fine_tune_checkpoint = '/content/models/mymodel/' + model_name + '/checkpoint/ckpt-0'\n",
        "\n",
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "print('Total classes:', num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eA5ht3_yukT"
      },
      "outputs": [],
      "source": [
        "# Create custom configuration file by writing the dataset, model checkpoint, and training parameters into the base pipeline file\n",
        "import re\n",
        "\n",
        "%cd /content/models/mymodel\n",
        "print('writing custom configuration file')\n",
        "\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "with open('pipeline_file.config', 'w') as f:\n",
        "    \n",
        "    # Set fine_tune_checkpoint path\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "    \n",
        "    # Set tfrecord files for train and test datasets\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(val_record_fname), s)\n",
        "\n",
        "    # Set label_map_path\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set batch_size\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "    \n",
        "    # Set number of classes num_classes\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "\n",
        "    # Change fine-tune checkpoint type from \"classification\" to \"detection\"\n",
        "    s = re.sub(\n",
        "        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
        "    \n",
        "    # If using ssd-mobilenet-v2, reduce learning rate (because it's too high in the default config file)\n",
        "    if chosen_model == 'ssd-mobilenet-v2':\n",
        "      s = re.sub('learning_rate_base: .8',\n",
        "                 'learning_rate_base: .08', s)\n",
        "      \n",
        "      s = re.sub('warmup_learning_rate: 0.13333',\n",
        "                 'warmup_learning_rate: .026666', s)\n",
        "    \n",
        "    # If using efficientdet-d0, use fixed_shape_resizer instead of keep_aspect_ratio_resizer (because it isn't supported by TFLite)\n",
        "    if chosen_model == 'efficientdet-d0':\n",
        "      s = re.sub('keep_aspect_ratio_resizer', 'fixed_shape_resizer', s)\n",
        "      s = re.sub('pad_to_max_dimension: true', '', s)\n",
        "      s = re.sub('min_dimension', 'height', s)\n",
        "      s = re.sub('max_dimension', 'width', s)\n",
        "\n",
        "    f.write(s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEsOLOMHzBqF"
      },
      "outputs": [],
      "source": [
        "# (Optional) Display the custom configuration file's contents\n",
        "!cat /content/models/mymodel/pipeline_file.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXpnXYC908Zl"
      },
      "source": [
        "Finally, let's set the locations of the configuration file and model output directory as variables so we can reference them when we call the training command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMlaN3rs3zLe"
      },
      "outputs": [],
      "source": [
        "# Set the path to the custom config file and the directory to store training checkpoints in\n",
        "pipeline_file = '/content/models/mymodel/pipeline_file.config'\n",
        "model_dir = '/content/training/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-19zML6oEO7l"
      },
      "source": [
        "# 4.&nbsp;Train Custom TFLite Detection Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBrjfGb_jRD4"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQTfZChVzzpZ",
        "outputId": "405e12f2-3fd6-4367-ebf7-17e2aa5bac9e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-01-19 08:47:48.372773: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-01-19 08:47:48.372993: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-01-19 08:47:48.373028: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-01-19 08:47:53.225962: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
            "W0119 08:47:53.230851 140093989418880 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
            "I0119 08:47:53.268527 140093989418880 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
            "INFO:tensorflow:Maybe overwriting train_steps: 30000\n",
            "I0119 08:47:53.275684 140093989418880 config_util.py:552] Maybe overwriting train_steps: 30000\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0119 08:47:53.275970 140093989418880 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "W0119 08:47:53.328644 140093989418880 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "INFO:tensorflow:Reading unweighted datasets: ['/content/train.tfrecord']\n",
            "I0119 08:47:53.345103 140093989418880 dataset_builder.py:162] Reading unweighted datasets: ['/content/train.tfrecord']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['/content/train.tfrecord']\n",
            "I0119 08:47:53.345483 140093989418880 dataset_builder.py:79] Reading record datasets for input file: ['/content/train.tfrecord']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I0119 08:47:53.345639 140093989418880 dataset_builder.py:80] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0119 08:47:53.345750 140093989418880 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "W0119 08:47:53.357743 140093989418880 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0119 08:47:53.393300 140093989418880 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "W0119 08:47:54.547984 140093989418880 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0119 08:48:02.081445 140093989418880 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W0119 08:48:05.461249 140093989418880 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0119 08:48:07.097677 140093989418880 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "2023-01-19 08:48:09.888428: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
            "/usr/local/lib/python3.8/dist-packages/keras/backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn(\n",
            "2023-01-19 08:48:44.128018: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "W0119 08:48:45.249515 140091612296960 deprecation.py:554] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "INFO:tensorflow:Step 100 per-step time 9.280s\n",
            "I0119 09:04:12.861650 140093989418880 model_lib_v2.py:705] Step 100 per-step time 9.280s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.24395214,\n",
            " 'Loss/localization_loss': 0.16156378,\n",
            " 'Loss/regularization_loss': 0.15329978,\n",
            " 'Loss/total_loss': 0.5588157,\n",
            " 'learning_rate': 0.0319994}\n",
            "I0119 09:04:12.862622 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.24395214,\n",
            " 'Loss/localization_loss': 0.16156378,\n",
            " 'Loss/regularization_loss': 0.15329978,\n",
            " 'Loss/total_loss': 0.5588157,\n",
            " 'learning_rate': 0.0319994}\n",
            "INFO:tensorflow:Step 200 per-step time 9.045s\n",
            "I0119 09:19:17.376453 140093989418880 model_lib_v2.py:705] Step 200 per-step time 9.045s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.17197946,\n",
            " 'Loss/localization_loss': 0.10592906,\n",
            " 'Loss/regularization_loss': 0.1530098,\n",
            " 'Loss/total_loss': 0.4309183,\n",
            " 'learning_rate': 0.0373328}\n",
            "I0119 09:19:17.377153 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.17197946,\n",
            " 'Loss/localization_loss': 0.10592906,\n",
            " 'Loss/regularization_loss': 0.1530098,\n",
            " 'Loss/total_loss': 0.4309183,\n",
            " 'learning_rate': 0.0373328}\n",
            "INFO:tensorflow:Step 300 per-step time 8.805s\n",
            "I0119 09:33:57.883268 140093989418880 model_lib_v2.py:705] Step 300 per-step time 8.805s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.15422325,\n",
            " 'Loss/localization_loss': 0.078076124,\n",
            " 'Loss/regularization_loss': 0.15268674,\n",
            " 'Loss/total_loss': 0.3849861,\n",
            " 'learning_rate': 0.0426662}\n",
            "I0119 09:33:57.883793 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.15422325,\n",
            " 'Loss/localization_loss': 0.078076124,\n",
            " 'Loss/regularization_loss': 0.15268674,\n",
            " 'Loss/total_loss': 0.3849861,\n",
            " 'learning_rate': 0.0426662}\n",
            "INFO:tensorflow:Step 400 per-step time 8.606s\n",
            "I0119 09:48:18.513761 140093989418880 model_lib_v2.py:705] Step 400 per-step time 8.606s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.16575837,\n",
            " 'Loss/localization_loss': 0.0877788,\n",
            " 'Loss/regularization_loss': 0.15235278,\n",
            " 'Loss/total_loss': 0.40588996,\n",
            " 'learning_rate': 0.047999598}\n",
            "I0119 09:48:18.514159 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.16575837,\n",
            " 'Loss/localization_loss': 0.0877788,\n",
            " 'Loss/regularization_loss': 0.15235278,\n",
            " 'Loss/total_loss': 0.40588996,\n",
            " 'learning_rate': 0.047999598}\n",
            "INFO:tensorflow:Step 500 per-step time 8.455s\n",
            "I0119 10:02:23.972270 140093989418880 model_lib_v2.py:705] Step 500 per-step time 8.455s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.14111328,\n",
            " 'Loss/localization_loss': 0.09517093,\n",
            " 'Loss/regularization_loss': 0.15198144,\n",
            " 'Loss/total_loss': 0.38826567,\n",
            " 'learning_rate': 0.053333}\n",
            "I0119 10:02:23.972752 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.14111328,\n",
            " 'Loss/localization_loss': 0.09517093,\n",
            " 'Loss/regularization_loss': 0.15198144,\n",
            " 'Loss/total_loss': 0.38826567,\n",
            " 'learning_rate': 0.053333}\n",
            "INFO:tensorflow:Step 600 per-step time 8.421s\n",
            "I0119 10:16:26.056441 140093989418880 model_lib_v2.py:705] Step 600 per-step time 8.421s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.110148415,\n",
            " 'Loss/localization_loss': 0.06797445,\n",
            " 'Loss/regularization_loss': 0.15161024,\n",
            " 'Loss/total_loss': 0.3297331,\n",
            " 'learning_rate': 0.0586664}\n",
            "I0119 10:16:26.057001 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.110148415,\n",
            " 'Loss/localization_loss': 0.06797445,\n",
            " 'Loss/regularization_loss': 0.15161024,\n",
            " 'Loss/total_loss': 0.3297331,\n",
            " 'learning_rate': 0.0586664}\n",
            "INFO:tensorflow:Step 700 per-step time 9.019s\n",
            "I0119 10:31:27.918108 140093989418880 model_lib_v2.py:705] Step 700 per-step time 9.019s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.12205174,\n",
            " 'Loss/localization_loss': 0.08500739,\n",
            " 'Loss/regularization_loss': 0.15117191,\n",
            " 'Loss/total_loss': 0.35823104,\n",
            " 'learning_rate': 0.0639998}\n",
            "I0119 10:31:27.918616 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.12205174,\n",
            " 'Loss/localization_loss': 0.08500739,\n",
            " 'Loss/regularization_loss': 0.15117191,\n",
            " 'Loss/total_loss': 0.35823104,\n",
            " 'learning_rate': 0.0639998}\n",
            "INFO:tensorflow:Step 800 per-step time 8.935s\n",
            "I0119 10:46:21.443238 140093989418880 model_lib_v2.py:705] Step 800 per-step time 8.935s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.105159216,\n",
            " 'Loss/localization_loss': 0.057465803,\n",
            " 'Loss/regularization_loss': 0.15071873,\n",
            " 'Loss/total_loss': 0.31334376,\n",
            " 'learning_rate': 0.069333196}\n",
            "I0119 10:46:21.443726 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.105159216,\n",
            " 'Loss/localization_loss': 0.057465803,\n",
            " 'Loss/regularization_loss': 0.15071873,\n",
            " 'Loss/total_loss': 0.31334376,\n",
            " 'learning_rate': 0.069333196}\n",
            "INFO:tensorflow:Step 900 per-step time 9.105s\n",
            "I0119 11:01:31.960679 140093989418880 model_lib_v2.py:705] Step 900 per-step time 9.105s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.11631611,\n",
            " 'Loss/localization_loss': 0.07766359,\n",
            " 'Loss/regularization_loss': 0.15019043,\n",
            " 'Loss/total_loss': 0.34417012,\n",
            " 'learning_rate': 0.074666604}\n",
            "I0119 11:01:31.961145 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.11631611,\n",
            " 'Loss/localization_loss': 0.07766359,\n",
            " 'Loss/regularization_loss': 0.15019043,\n",
            " 'Loss/total_loss': 0.34417012,\n",
            " 'learning_rate': 0.074666604}\n",
            "INFO:tensorflow:Step 1000 per-step time 8.738s\n",
            "I0119 11:16:05.752747 140093989418880 model_lib_v2.py:705] Step 1000 per-step time 8.738s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.10699108,\n",
            " 'Loss/localization_loss': 0.08353311,\n",
            " 'Loss/regularization_loss': 0.14968355,\n",
            " 'Loss/total_loss': 0.34020776,\n",
            " 'learning_rate': 0.08}\n",
            "I0119 11:16:05.753180 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.10699108,\n",
            " 'Loss/localization_loss': 0.08353311,\n",
            " 'Loss/regularization_loss': 0.14968355,\n",
            " 'Loss/total_loss': 0.34020776,\n",
            " 'learning_rate': 0.08}\n",
            "INFO:tensorflow:Step 1100 per-step time 8.703s\n",
            "I0119 11:30:36.032212 140093989418880 model_lib_v2.py:705] Step 1100 per-step time 8.703s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.089961074,\n",
            " 'Loss/localization_loss': 0.0602511,\n",
            " 'Loss/regularization_loss': 0.149106,\n",
            " 'Loss/total_loss': 0.29931816,\n",
            " 'learning_rate': 0.07999918}\n",
            "I0119 11:30:36.032670 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.089961074,\n",
            " 'Loss/localization_loss': 0.0602511,\n",
            " 'Loss/regularization_loss': 0.149106,\n",
            " 'Loss/total_loss': 0.29931816,\n",
            " 'learning_rate': 0.07999918}\n",
            "INFO:tensorflow:Step 1200 per-step time 8.310s\n",
            "I0119 11:44:27.015583 140093989418880 model_lib_v2.py:705] Step 1200 per-step time 8.310s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.09630797,\n",
            " 'Loss/localization_loss': 0.04702393,\n",
            " 'Loss/regularization_loss': 0.14853527,\n",
            " 'Loss/total_loss': 0.29186717,\n",
            " 'learning_rate': 0.079996705}\n",
            "I0119 11:44:27.016093 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.09630797,\n",
            " 'Loss/localization_loss': 0.04702393,\n",
            " 'Loss/regularization_loss': 0.14853527,\n",
            " 'Loss/total_loss': 0.29186717,\n",
            " 'learning_rate': 0.079996705}\n",
            "INFO:tensorflow:Step 1300 per-step time 8.520s\n",
            "I0119 11:58:38.974326 140093989418880 model_lib_v2.py:705] Step 1300 per-step time 8.520s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.07944628,\n",
            " 'Loss/localization_loss': 0.039611686,\n",
            " 'Loss/regularization_loss': 0.14793484,\n",
            " 'Loss/total_loss': 0.2669928,\n",
            " 'learning_rate': 0.0799926}\n",
            "I0119 11:58:38.974821 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.07944628,\n",
            " 'Loss/localization_loss': 0.039611686,\n",
            " 'Loss/regularization_loss': 0.14793484,\n",
            " 'Loss/total_loss': 0.2669928,\n",
            " 'learning_rate': 0.0799926}\n",
            "INFO:tensorflow:Step 1400 per-step time 9.128s\n",
            "I0119 12:13:51.753418 140093989418880 model_lib_v2.py:705] Step 1400 per-step time 9.128s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.08966611,\n",
            " 'Loss/localization_loss': 0.060984634,\n",
            " 'Loss/regularization_loss': 0.14722179,\n",
            " 'Loss/total_loss': 0.29787254,\n",
            " 'learning_rate': 0.07998685}\n",
            "I0119 12:13:51.753900 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.08966611,\n",
            " 'Loss/localization_loss': 0.060984634,\n",
            " 'Loss/regularization_loss': 0.14722179,\n",
            " 'Loss/total_loss': 0.29787254,\n",
            " 'learning_rate': 0.07998685}\n",
            "INFO:tensorflow:Step 1500 per-step time 8.691s\n",
            "I0119 12:28:20.856176 140093989418880 model_lib_v2.py:705] Step 1500 per-step time 8.691s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.08869551,\n",
            " 'Loss/localization_loss': 0.03611959,\n",
            " 'Loss/regularization_loss': 0.14659314,\n",
            " 'Loss/total_loss': 0.27140826,\n",
            " 'learning_rate': 0.07997945}\n",
            "I0119 12:28:20.856623 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.08869551,\n",
            " 'Loss/localization_loss': 0.03611959,\n",
            " 'Loss/regularization_loss': 0.14659314,\n",
            " 'Loss/total_loss': 0.27140826,\n",
            " 'learning_rate': 0.07997945}\n",
            "INFO:tensorflow:Step 1600 per-step time 8.553s\n",
            "I0119 12:42:36.148946 140093989418880 model_lib_v2.py:705] Step 1600 per-step time 8.553s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.086846314,\n",
            " 'Loss/localization_loss': 0.048681956,\n",
            " 'Loss/regularization_loss': 0.14606358,\n",
            " 'Loss/total_loss': 0.28159186,\n",
            " 'learning_rate': 0.079970405}\n",
            "I0119 12:42:36.149417 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.086846314,\n",
            " 'Loss/localization_loss': 0.048681956,\n",
            " 'Loss/regularization_loss': 0.14606358,\n",
            " 'Loss/total_loss': 0.28159186,\n",
            " 'learning_rate': 0.079970405}\n",
            "INFO:tensorflow:Step 1700 per-step time 8.454s\n",
            "I0119 12:56:41.505618 140093989418880 model_lib_v2.py:705] Step 1700 per-step time 8.454s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.07589898,\n",
            " 'Loss/localization_loss': 0.035733934,\n",
            " 'Loss/regularization_loss': 0.14535467,\n",
            " 'Loss/total_loss': 0.2569876,\n",
            " 'learning_rate': 0.07995972}\n",
            "I0119 12:56:41.506317 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.07589898,\n",
            " 'Loss/localization_loss': 0.035733934,\n",
            " 'Loss/regularization_loss': 0.14535467,\n",
            " 'Loss/total_loss': 0.2569876,\n",
            " 'learning_rate': 0.07995972}\n",
            "INFO:tensorflow:Step 1800 per-step time 8.615s\n",
            "I0119 13:11:03.032254 140093989418880 model_lib_v2.py:705] Step 1800 per-step time 8.615s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.06766229,\n",
            " 'Loss/localization_loss': 0.03712859,\n",
            " 'Loss/regularization_loss': 0.14467783,\n",
            " 'Loss/total_loss': 0.24946871,\n",
            " 'learning_rate': 0.0799474}\n",
            "I0119 13:11:03.032788 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.06766229,\n",
            " 'Loss/localization_loss': 0.03712859,\n",
            " 'Loss/regularization_loss': 0.14467783,\n",
            " 'Loss/total_loss': 0.24946871,\n",
            " 'learning_rate': 0.0799474}\n",
            "INFO:tensorflow:Step 1900 per-step time 8.788s\n",
            "I0119 13:25:41.847291 140093989418880 model_lib_v2.py:705] Step 1900 per-step time 8.788s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.06749034,\n",
            " 'Loss/localization_loss': 0.028168464,\n",
            " 'Loss/regularization_loss': 0.1440368,\n",
            " 'Loss/total_loss': 0.23969561,\n",
            " 'learning_rate': 0.07993342}\n",
            "I0119 13:25:41.847786 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.06749034,\n",
            " 'Loss/localization_loss': 0.028168464,\n",
            " 'Loss/regularization_loss': 0.1440368,\n",
            " 'Loss/total_loss': 0.23969561,\n",
            " 'learning_rate': 0.07993342}\n",
            "INFO:tensorflow:Step 2000 per-step time 8.683s\n",
            "I0119 13:40:10.153834 140093989418880 model_lib_v2.py:705] Step 2000 per-step time 8.683s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.07103314,\n",
            " 'Loss/localization_loss': 0.03357003,\n",
            " 'Loss/regularization_loss': 0.1433404,\n",
            " 'Loss/total_loss': 0.24794358,\n",
            " 'learning_rate': 0.07991781}\n",
            "I0119 13:40:10.154324 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.07103314,\n",
            " 'Loss/localization_loss': 0.03357003,\n",
            " 'Loss/regularization_loss': 0.1433404,\n",
            " 'Loss/total_loss': 0.24794358,\n",
            " 'learning_rate': 0.07991781}\n",
            "INFO:tensorflow:Step 2100 per-step time 8.693s\n",
            "I0119 13:54:39.501736 140093989418880 model_lib_v2.py:705] Step 2100 per-step time 8.693s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.072103076,\n",
            " 'Loss/localization_loss': 0.033614323,\n",
            " 'Loss/regularization_loss': 0.14260696,\n",
            " 'Loss/total_loss': 0.24832436,\n",
            " 'learning_rate': 0.07990056}\n",
            "I0119 13:54:39.502367 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.072103076,\n",
            " 'Loss/localization_loss': 0.033614323,\n",
            " 'Loss/regularization_loss': 0.14260696,\n",
            " 'Loss/total_loss': 0.24832436,\n",
            " 'learning_rate': 0.07990056}\n",
            "INFO:tensorflow:Step 2200 per-step time 8.811s\n",
            "I0119 14:09:20.571523 140093989418880 model_lib_v2.py:705] Step 2200 per-step time 8.811s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.067752846,\n",
            " 'Loss/localization_loss': 0.040675852,\n",
            " 'Loss/regularization_loss': 0.14186738,\n",
            " 'Loss/total_loss': 0.2502961,\n",
            " 'learning_rate': 0.07988167}\n",
            "I0119 14:09:20.571993 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.067752846,\n",
            " 'Loss/localization_loss': 0.040675852,\n",
            " 'Loss/regularization_loss': 0.14186738,\n",
            " 'Loss/total_loss': 0.2502961,\n",
            " 'learning_rate': 0.07988167}\n",
            "INFO:tensorflow:Step 2300 per-step time 8.848s\n",
            "I0119 14:24:05.383821 140093989418880 model_lib_v2.py:705] Step 2300 per-step time 8.848s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.06179464,\n",
            " 'Loss/localization_loss': 0.02742999,\n",
            " 'Loss/regularization_loss': 0.14115767,\n",
            " 'Loss/total_loss': 0.2303823,\n",
            " 'learning_rate': 0.07986114}\n",
            "I0119 14:24:05.384265 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.06179464,\n",
            " 'Loss/localization_loss': 0.02742999,\n",
            " 'Loss/regularization_loss': 0.14115767,\n",
            " 'Loss/total_loss': 0.2303823,\n",
            " 'learning_rate': 0.07986114}\n",
            "INFO:tensorflow:Step 2400 per-step time 9.005s\n",
            "I0119 14:39:05.883247 140093989418880 model_lib_v2.py:705] Step 2400 per-step time 9.005s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.081816405,\n",
            " 'Loss/localization_loss': 0.03980448,\n",
            " 'Loss/regularization_loss': 0.14062811,\n",
            " 'Loss/total_loss': 0.262249,\n",
            " 'learning_rate': 0.07983897}\n",
            "I0119 14:39:05.883747 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.081816405,\n",
            " 'Loss/localization_loss': 0.03980448,\n",
            " 'Loss/regularization_loss': 0.14062811,\n",
            " 'Loss/total_loss': 0.262249,\n",
            " 'learning_rate': 0.07983897}\n",
            "INFO:tensorflow:Step 2500 per-step time 9.017s\n",
            "I0119 14:54:07.585963 140093989418880 model_lib_v2.py:705] Step 2500 per-step time 9.017s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.076366454,\n",
            " 'Loss/localization_loss': 0.035671156,\n",
            " 'Loss/regularization_loss': 0.1400444,\n",
            " 'Loss/total_loss': 0.25208202,\n",
            " 'learning_rate': 0.079815164}\n",
            "I0119 14:54:07.586548 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.076366454,\n",
            " 'Loss/localization_loss': 0.035671156,\n",
            " 'Loss/regularization_loss': 0.1400444,\n",
            " 'Loss/total_loss': 0.25208202,\n",
            " 'learning_rate': 0.079815164}\n",
            "INFO:tensorflow:Step 2600 per-step time 8.984s\n",
            "I0119 15:09:06.019844 140093989418880 model_lib_v2.py:705] Step 2600 per-step time 8.984s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.07144702,\n",
            " 'Loss/localization_loss': 0.033632684,\n",
            " 'Loss/regularization_loss': 0.13931057,\n",
            " 'Loss/total_loss': 0.24439028,\n",
            " 'learning_rate': 0.07978972}\n",
            "I0119 15:09:06.020328 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.07144702,\n",
            " 'Loss/localization_loss': 0.033632684,\n",
            " 'Loss/regularization_loss': 0.13931057,\n",
            " 'Loss/total_loss': 0.24439028,\n",
            " 'learning_rate': 0.07978972}\n",
            "INFO:tensorflow:Step 2700 per-step time 9.082s\n",
            "I0119 15:24:14.206477 140093989418880 model_lib_v2.py:705] Step 2700 per-step time 9.082s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.05764613,\n",
            " 'Loss/localization_loss': 0.027459161,\n",
            " 'Loss/regularization_loss': 0.13860211,\n",
            " 'Loss/total_loss': 0.2237074,\n",
            " 'learning_rate': 0.07976264}\n",
            "I0119 15:24:14.206965 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.05764613,\n",
            " 'Loss/localization_loss': 0.027459161,\n",
            " 'Loss/regularization_loss': 0.13860211,\n",
            " 'Loss/total_loss': 0.2237074,\n",
            " 'learning_rate': 0.07976264}\n",
            "INFO:tensorflow:Step 2800 per-step time 9.171s\n",
            "I0119 15:39:31.293879 140093989418880 model_lib_v2.py:705] Step 2800 per-step time 9.171s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.056557477,\n",
            " 'Loss/localization_loss': 0.029590603,\n",
            " 'Loss/regularization_loss': 0.13789034,\n",
            " 'Loss/total_loss': 0.22403842,\n",
            " 'learning_rate': 0.07973392}\n",
            "I0119 15:39:31.294447 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.056557477,\n",
            " 'Loss/localization_loss': 0.029590603,\n",
            " 'Loss/regularization_loss': 0.13789034,\n",
            " 'Loss/total_loss': 0.22403842,\n",
            " 'learning_rate': 0.07973392}\n",
            "INFO:tensorflow:Step 2900 per-step time 8.949s\n",
            "I0119 15:54:26.159638 140093989418880 model_lib_v2.py:705] Step 2900 per-step time 8.949s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.05664366,\n",
            " 'Loss/localization_loss': 0.021375202,\n",
            " 'Loss/regularization_loss': 0.13717353,\n",
            " 'Loss/total_loss': 0.2151924,\n",
            " 'learning_rate': 0.07970358}\n",
            "I0119 15:54:26.160095 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.05664366,\n",
            " 'Loss/localization_loss': 0.021375202,\n",
            " 'Loss/regularization_loss': 0.13717353,\n",
            " 'Loss/total_loss': 0.2151924,\n",
            " 'learning_rate': 0.07970358}\n",
            "INFO:tensorflow:Step 3000 per-step time 8.820s\n",
            "I0119 16:09:08.150665 140093989418880 model_lib_v2.py:705] Step 3000 per-step time 8.820s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.061370637,\n",
            " 'Loss/localization_loss': 0.026853958,\n",
            " 'Loss/regularization_loss': 0.13649607,\n",
            " 'Loss/total_loss': 0.22472067,\n",
            " 'learning_rate': 0.0796716}\n",
            "I0119 16:09:08.151125 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.061370637,\n",
            " 'Loss/localization_loss': 0.026853958,\n",
            " 'Loss/regularization_loss': 0.13649607,\n",
            " 'Loss/total_loss': 0.22472067,\n",
            " 'learning_rate': 0.0796716}\n",
            "INFO:tensorflow:Step 3100 per-step time 8.857s\n",
            "I0119 16:23:53.882023 140093989418880 model_lib_v2.py:705] Step 3100 per-step time 8.857s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.06794213,\n",
            " 'Loss/localization_loss': 0.023202326,\n",
            " 'Loss/regularization_loss': 0.13578911,\n",
            " 'Loss/total_loss': 0.22693357,\n",
            " 'learning_rate': 0.07963799}\n",
            "I0119 16:23:53.882628 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.06794213,\n",
            " 'Loss/localization_loss': 0.023202326,\n",
            " 'Loss/regularization_loss': 0.13578911,\n",
            " 'Loss/total_loss': 0.22693357,\n",
            " 'learning_rate': 0.07963799}\n",
            "INFO:tensorflow:Step 3200 per-step time 8.812s\n",
            "I0119 16:38:35.051566 140093989418880 model_lib_v2.py:705] Step 3200 per-step time 8.812s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.065412395,\n",
            " 'Loss/localization_loss': 0.031491417,\n",
            " 'Loss/regularization_loss': 0.13518564,\n",
            " 'Loss/total_loss': 0.23208946,\n",
            " 'learning_rate': 0.07960275}\n",
            "I0119 16:38:35.052029 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.065412395,\n",
            " 'Loss/localization_loss': 0.031491417,\n",
            " 'Loss/regularization_loss': 0.13518564,\n",
            " 'Loss/total_loss': 0.23208946,\n",
            " 'learning_rate': 0.07960275}\n",
            "INFO:tensorflow:Step 3300 per-step time 9.018s\n",
            "I0119 16:53:36.821459 140093989418880 model_lib_v2.py:705] Step 3300 per-step time 9.018s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.06376919,\n",
            " 'Loss/localization_loss': 0.028804407,\n",
            " 'Loss/regularization_loss': 0.13504851,\n",
            " 'Loss/total_loss': 0.2276221,\n",
            " 'learning_rate': 0.07956588}\n",
            "I0119 16:53:36.821941 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.06376919,\n",
            " 'Loss/localization_loss': 0.028804407,\n",
            " 'Loss/regularization_loss': 0.13504851,\n",
            " 'Loss/total_loss': 0.2276221,\n",
            " 'learning_rate': 0.07956588}\n",
            "INFO:tensorflow:Step 3400 per-step time 9.132s\n",
            "I0119 17:08:49.987834 140093989418880 model_lib_v2.py:705] Step 3400 per-step time 9.132s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.055656664,\n",
            " 'Loss/localization_loss': 0.03302681,\n",
            " 'Loss/regularization_loss': 0.13450028,\n",
            " 'Loss/total_loss': 0.22318375,\n",
            " 'learning_rate': 0.079527386}\n",
            "I0119 17:08:49.988326 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.055656664,\n",
            " 'Loss/localization_loss': 0.03302681,\n",
            " 'Loss/regularization_loss': 0.13450028,\n",
            " 'Loss/total_loss': 0.22318375,\n",
            " 'learning_rate': 0.079527386}\n",
            "INFO:tensorflow:Step 3500 per-step time 8.920s\n",
            "I0119 17:23:42.026125 140093989418880 model_lib_v2.py:705] Step 3500 per-step time 8.920s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.06480931,\n",
            " 'Loss/localization_loss': 0.025798585,\n",
            " 'Loss/regularization_loss': 0.1337774,\n",
            " 'Loss/total_loss': 0.22438529,\n",
            " 'learning_rate': 0.07948727}\n",
            "I0119 17:23:42.026574 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.06480931,\n",
            " 'Loss/localization_loss': 0.025798585,\n",
            " 'Loss/regularization_loss': 0.1337774,\n",
            " 'Loss/total_loss': 0.22438529,\n",
            " 'learning_rate': 0.07948727}\n",
            "INFO:tensorflow:Step 3600 per-step time 9.099s\n",
            "I0119 17:38:51.895631 140093989418880 model_lib_v2.py:705] Step 3600 per-step time 9.099s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.05428378,\n",
            " 'Loss/localization_loss': 0.039705135,\n",
            " 'Loss/regularization_loss': 0.13344215,\n",
            " 'Loss/total_loss': 0.22743107,\n",
            " 'learning_rate': 0.079445526}\n",
            "I0119 17:38:51.896108 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.05428378,\n",
            " 'Loss/localization_loss': 0.039705135,\n",
            " 'Loss/regularization_loss': 0.13344215,\n",
            " 'Loss/total_loss': 0.22743107,\n",
            " 'learning_rate': 0.079445526}\n",
            "INFO:tensorflow:Step 3700 per-step time 9.079s\n",
            "I0119 17:53:59.754855 140093989418880 model_lib_v2.py:705] Step 3700 per-step time 9.079s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.053543407,\n",
            " 'Loss/localization_loss': 0.021618355,\n",
            " 'Loss/regularization_loss': 0.1327873,\n",
            " 'Loss/total_loss': 0.20794906,\n",
            " 'learning_rate': 0.07940216}\n",
            "I0119 17:53:59.755302 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.053543407,\n",
            " 'Loss/localization_loss': 0.021618355,\n",
            " 'Loss/regularization_loss': 0.1327873,\n",
            " 'Loss/total_loss': 0.20794906,\n",
            " 'learning_rate': 0.07940216}\n",
            "INFO:tensorflow:Step 3800 per-step time 9.356s\n",
            "I0119 18:09:35.326514 140093989418880 model_lib_v2.py:705] Step 3800 per-step time 9.356s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.058214907,\n",
            " 'Loss/localization_loss': 0.027370187,\n",
            " 'Loss/regularization_loss': 0.13210684,\n",
            " 'Loss/total_loss': 0.21769193,\n",
            " 'learning_rate': 0.079357184}\n",
            "I0119 18:09:35.327064 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.058214907,\n",
            " 'Loss/localization_loss': 0.027370187,\n",
            " 'Loss/regularization_loss': 0.13210684,\n",
            " 'Loss/total_loss': 0.21769193,\n",
            " 'learning_rate': 0.079357184}\n",
            "INFO:tensorflow:Step 3900 per-step time 10.251s\n",
            "I0119 18:26:40.438466 140093989418880 model_lib_v2.py:705] Step 3900 per-step time 10.251s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.05907144,\n",
            " 'Loss/localization_loss': 0.022981925,\n",
            " 'Loss/regularization_loss': 0.13140403,\n",
            " 'Loss/total_loss': 0.21345739,\n",
            " 'learning_rate': 0.07931058}\n",
            "I0119 18:26:40.438920 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.05907144,\n",
            " 'Loss/localization_loss': 0.022981925,\n",
            " 'Loss/regularization_loss': 0.13140403,\n",
            " 'Loss/total_loss': 0.21345739,\n",
            " 'learning_rate': 0.07931058}\n",
            "INFO:tensorflow:Step 4000 per-step time 9.168s\n",
            "I0119 18:41:57.229262 140093989418880 model_lib_v2.py:705] Step 4000 per-step time 9.168s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.05286949,\n",
            " 'Loss/localization_loss': 0.025240896,\n",
            " 'Loss/regularization_loss': 0.13081725,\n",
            " 'Loss/total_loss': 0.20892763,\n",
            " 'learning_rate': 0.07926236}\n",
            "I0119 18:41:57.229761 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.05286949,\n",
            " 'Loss/localization_loss': 0.025240896,\n",
            " 'Loss/regularization_loss': 0.13081725,\n",
            " 'Loss/total_loss': 0.20892763,\n",
            " 'learning_rate': 0.07926236}\n",
            "INFO:tensorflow:Step 4100 per-step time 9.233s\n",
            "I0119 18:57:20.516368 140093989418880 model_lib_v2.py:705] Step 4100 per-step time 9.233s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.05953731,\n",
            " 'Loss/localization_loss': 0.024012027,\n",
            " 'Loss/regularization_loss': 0.13013339,\n",
            " 'Loss/total_loss': 0.21368273,\n",
            " 'learning_rate': 0.07921253}\n",
            "I0119 18:57:20.516938 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.05953731,\n",
            " 'Loss/localization_loss': 0.024012027,\n",
            " 'Loss/regularization_loss': 0.13013339,\n",
            " 'Loss/total_loss': 0.21368273,\n",
            " 'learning_rate': 0.07921253}\n",
            "INFO:tensorflow:Step 4200 per-step time 9.167s\n",
            "I0119 19:12:37.230414 140093989418880 model_lib_v2.py:705] Step 4200 per-step time 9.167s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.05805129,\n",
            " 'Loss/localization_loss': 0.02454909,\n",
            " 'Loss/regularization_loss': 0.12942104,\n",
            " 'Loss/total_loss': 0.21202141,\n",
            " 'learning_rate': 0.07916109}\n",
            "I0119 19:12:37.230872 140093989418880 model_lib_v2.py:708] {'Loss/classification_loss': 0.05805129,\n",
            " 'Loss/localization_loss': 0.02454909,\n",
            " 'Loss/regularization_loss': 0.12942104,\n",
            " 'Loss/total_loss': 0.21202141,\n",
            " 'learning_rate': 0.07916109}\n"
          ]
        }
      ],
      "source": [
        "# Run training!\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={pipeline_file} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --sample_1_of_n_eval_examples=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHxbX4ZpzXIv"
      },
      "source": [
        "If you want to stop training early, just click Stop a couple times or right-click on the code block and select \"Interrupt Execution\". Otherwise, training will stop by itself once it reaches the specified number of training steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPg8oMnQDYKl"
      },
      "source": [
        "# 5.&nbsp;Convert Model to TensorFlow Lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaUU8tBlHifd"
      },
      "outputs": [],
      "source": [
        "# Make a directory to store the trained TFLite model\n",
        "!mkdir /content/drive/MyDrive/Colab_Notebooks/custom_model_lite\n",
        "output_directory = '/content/drive/MyDrive/Colab_Notebooks/custom_model_lite'\n",
        "\n",
        "#  Path to training directory (the conversion script automatically chooses the highest checkpoint file)\n",
        "last_model_path = '/content/training'\n",
        "\n",
        "!python /content/models/research/object_detection/export_tflite_graph_tf2.py \\\n",
        "    --trained_checkpoint_dir {last_model_path} \\\n",
        "    --output_directory {output_directory} \\\n",
        "    --pipeline_config_path {pipeline_file}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_NuapO2VROu"
      },
      "source": [
        "Next, we'll take the exported graph and use the `TFLiteConverter` module to convert it to `.tflite` FlatBuffer format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsE_uVjlsz3u"
      },
      "outputs": [],
      "source": [
        "# Convert exported graph file into TFLite model file\n",
        "import tensorflow as tf\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('/content/drive/MyDrive/Colab_Notebooks/custom_model_lite/saved_model')\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/custom_model_lite/detect2.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDQrtQhvC3oG"
      },
      "source": [
        "# 6.&nbsp;Test TensorFlow Lite Model and Calculate mAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4WtI8i5K96w"
      },
      "outputs": [],
      "source": [
        "# Script to run custom TFLite model on test images to detect objects\n",
        "# Source: https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py\n",
        "\n",
        "# Import packages\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import glob\n",
        "import random\n",
        "import importlib.util\n",
        "from tensorflow.lite.python.interpreter import Interpreter\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "### Define function for inferencing with TFLite model and displaying results\n",
        "\n",
        "def tflite_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=10, savepath='/content/results', txt_only=False):\n",
        "\n",
        "  # Grab filenames of all images in test folder\n",
        "  images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n",
        "\n",
        "  # Load the label map into memory\n",
        "  with open(lblpath, 'r') as f:\n",
        "      labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "  # Load the Tensorflow Lite model into memory\n",
        "  interpreter = Interpreter(model_path=modelpath)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  # Get model details\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "  height = input_details[0]['shape'][1]\n",
        "  width = input_details[0]['shape'][2]\n",
        "\n",
        "  float_input = (input_details[0]['dtype'] == np.float32)\n",
        "\n",
        "  input_mean = 127.5\n",
        "  input_std = 127.5\n",
        "\n",
        "  # Randomly select test images\n",
        "  images_to_test = random.sample(images, num_test_images)\n",
        "\n",
        "  # Loop over every image and perform detection\n",
        "  for image_path in images_to_test:\n",
        "\n",
        "      # Load image and resize to expected shape [1xHxWx3]\n",
        "      image = cv2.imread(image_path)\n",
        "      image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      imH, imW, _ = image.shape \n",
        "      image_resized = cv2.resize(image_rgb, (width, height))\n",
        "      input_data = np.expand_dims(image_resized, axis=0)\n",
        "\n",
        "      # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n",
        "      if float_input:\n",
        "          input_data = (np.float32(input_data) - input_mean) / input_std\n",
        "\n",
        "      # Perform the actual detection by running the model with the image as input\n",
        "      interpreter.set_tensor(input_details[0]['index'],input_data)\n",
        "      interpreter.invoke()\n",
        "\n",
        "      # Retrieve detection results\n",
        "      boxes = interpreter.get_tensor(output_details[1]['index'])[0] # Bounding box coordinates of detected objects\n",
        "      classes = interpreter.get_tensor(output_details[3]['index'])[0] # Class index of detected objects\n",
        "      scores = interpreter.get_tensor(output_details[0]['index'])[0] # Confidence of detected objects\n",
        "\n",
        "      detections = []\n",
        "\n",
        "      # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
        "      for i in range(len(scores)):\n",
        "          if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n",
        "\n",
        "              # Get bounding box coordinates and draw box\n",
        "              # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n",
        "              ymin = int(max(1,(boxes[i][0] * imH)))\n",
        "              xmin = int(max(1,(boxes[i][1] * imW)))\n",
        "              ymax = int(min(imH,(boxes[i][2] * imH)))\n",
        "              xmax = int(min(imW,(boxes[i][3] * imW)))\n",
        "              \n",
        "              cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n",
        "\n",
        "              # Draw label\n",
        "              object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n",
        "              label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n",
        "              labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
        "              label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
        "              cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n",
        "              cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
        "\n",
        "              detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n",
        "\n",
        "      \n",
        "      # All the results have been drawn on the image, now display the image\n",
        "      if txt_only == False: # \"text_only\" controls whether we want to display the image results or just save them in .txt files\n",
        "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "        plt.figure(figsize=(12,16))\n",
        "        plt.imshow(image)\n",
        "        plt.show()\n",
        "      \n",
        "      # Save detection results in .txt files (for calculating mAP)\n",
        "      elif txt_only == True:\n",
        "\n",
        "        # Get filenames and paths\n",
        "        image_fn = os.path.basename(image_path)      \n",
        "        base_fn, ext = os.path.splitext(image_fn)\n",
        "        txt_result_fn = base_fn +'.txt'\n",
        "        txt_savepath = os.path.join(savepath, txt_result_fn)\n",
        "\n",
        "        # Write results to text file\n",
        "        # (Using format defined by https://github.com/Cartucho/mAP, which will make it easy to calculate mAP)\n",
        "        with open(txt_savepath,'w') as f:\n",
        "            for detection in detections:\n",
        "                f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CJI4A0f_zqz"
      },
      "source": [
        "The next block sets the paths to the test images and models and then runs the inferencing function. If you want to use more than 10 images, change the `images_to_test` variable. Click play to run inferencing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t8CMarqBqP9"
      },
      "outputs": [],
      "source": [
        "# Set up variables for running user's model\n",
        "PATH_TO_IMAGES='/content/images/test'   # Path to test images folder\n",
        "PATH_TO_MODEL='/content/drive/MyDrive/Colab_Notebooks/custom_model_lite/detect.tflite'   # Path to .tflite model file\n",
        "PATH_TO_LABELS='/content/drive/MyDrive/Colab_Notebooks/labelmap.txt'   # Path to labelmap.txt file\n",
        "min_conf_threshold=0.5   # Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n",
        "images_to_test = 10  # Number of images to run detection on\n",
        "\n",
        "# Run inferencing function!\n",
        "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_O6gjKxjgR9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "8oNuf3AVi0Jn",
        "outputId": "39efc7e4-82a6-4670-8d02-aae32e56d7e4"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b5a806590972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstop_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcurrent_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current Time =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "stop_time = time.time()\n",
        "current_time = now.strftime(\"%H:%M:%S\")\n",
        "print(\"Current Time =\", current_time)\n",
        "print(\"Done in \",stop_time-start_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_ckqeWqBF0P"
      },
      "source": [
        "### 6.1 Calculate mAP\n",
        "Now we have a visual sense of how our model performs on test images, but how can we quantitatively measure its accuracy? \n",
        "\n",
        "One popular methord for measuring object detection model accuracy is \"mean average precision\" (mAP). Basically, the higher the mAP score, the better your model is at detecting objects in images. To learn more about mAP, read through this [article from Roboflow](https://blog.roboflow.com/mean-average-precision/).\n",
        "\n",
        "We'll use the mAP calculator tool at https://github.com/Cartucho/mAP to determine our model's mAP score. First, we need to clone the repository and remove its existing example data. We'll also download a script I wrote for interfacing with the calculator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlWarXEZDUqS",
        "outputId": "40036917-5eaf-49ab-f0a3-faa81cbdb3a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into '/content/mAP'...\n",
            "--2023-01-19 08:35:44--  https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/calculate_map_cartucho.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5397 (5.3K) [text/plain]\n",
            "Saving to: ‘calculate_map_cartucho.py’\n",
            "\n",
            "     0K .....                                                 100% 26.1M=0s\n",
            "\n",
            "2023-01-19 08:35:44 (26.1 MB/s) - ‘calculate_map_cartucho.py’ saved [5397/5397]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "git clone https://github.com/Cartucho/mAP /content/mAP\n",
        "cd /content/mAP\n",
        "rm input/detection-results/* \n",
        "rm input/ground-truth/* \n",
        "rm input/images-optional/* \n",
        "wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/calculate_map_cartucho.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn22nGGqH5T6"
      },
      "source": [
        "Next, we'll copy the images and annotation data from the **test** folder to the appropriate folders inside the cloned repository. These will be used as the \"ground truth data\" that our model's detection results will be compared to.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5szFfVxwI3wT"
      },
      "outputs": [],
      "source": [
        "!cp /content/images/test/* /content/mAP/input/images-optional # Copy images and xml files\n",
        "!mv /content/mAP/input/images-optional/*.xml /content/mAP/input/ground-truth/  # Move xml files to the appropriate folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6aro817DGzx"
      },
      "source": [
        "The calculator tool expects annotation data in a format that's different from the Pascal VOC .xml file format we're using. Fortunately, it provides an easy script, `convert_gt_xml.py`, for converting to the expected .txt format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdjtOUDnK2AA",
        "outputId": "dca93987-2df5-499c-bc71-7df5fb9f89b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversion completed!\n"
          ]
        }
      ],
      "source": [
        "!python /content/mAP/scripts/extra/convert_gt_xml.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnIUacAlLP0B"
      },
      "source": [
        "Okay, we've set up the ground truth data, but now we need actual detection results from our model. The detection results will be compared to the ground truth data to calculate the model's accuracy in mAP.\n",
        "\n",
        "The inference function we defined in Step 7.1 can be used to generate detection data for all the images in the **test** folder. We'll use it the same as before, except this time we'll tell it to save detection results into the `detection-results` folder.\n",
        "\n",
        "Click Play to run the following code block!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szzHFAhsMNFF",
        "outputId": "dde4cb1a-b8a0-46c1-aea4-3c6a4bf3e21c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting inference on 16 images...\n",
            "Finished inferencing!\n"
          ]
        }
      ],
      "source": [
        "# Set up variables for running inference, this time to get detection results saved as .txt files\n",
        "PATH_TO_IMAGES='/content/images/test'   # Path to test images folder\n",
        "PATH_TO_MODEL='/content/drive/MyDrive/Colab_Notebooks/custom_model_lite/detect.tflite'   # Path to .tflite model file\n",
        "PATH_TO_LABELS='/content/labelmap.txt'   # Path to labelmap.txt file\n",
        "PATH_TO_RESULTS='/content/mAP/input/detection-results' # Folder to save detection results in\n",
        "min_conf_threshold=0.1   # Confidence threshold\n",
        "\n",
        "# Use all the images in the test folder\n",
        "image_list = glob.glob(PATH_TO_IMAGES + '/*.jpg') + glob.glob(PATH_TO_IMAGES + '/*.JPG') + glob.glob(PATH_TO_IMAGES + '/*.png') + glob.glob(PATH_TO_IMAGES + '/*.bmp')\n",
        "images_to_test = min(500, len(image_list)) # If there are more than 500 images in the folder, just use 500\n",
        "\n",
        "# Tell function to just save results and not display images\n",
        "txt_only = True\n",
        "\n",
        "# Run inferencing function!\n",
        "print('Starting inference on %d images...' % images_to_test)\n",
        "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test, PATH_TO_RESULTS, txt_only)\n",
        "print('Finished inferencing!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_QRnTqNPX4z"
      },
      "source": [
        "Finally, let's calculate mAP! One popular style for reporting mAP is the COCO metric for mAP @ 0.50:0.95. Basically, this means that mAP is calculated at several IoU thresholds between 0.50 and 0.95, and then the result from each threshold is averaged to get a final mAP score. [Learn more here!](https://blog.roboflow.com/mean-average-precision/)\n",
        "\n",
        "I wrote a script to run the calculator tool at each IoU threshold, average the results, and report the final accuracy score. It reports mAP for each class and overall mAP. Click Play on the following two blocks to calculate mAP!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHza3t9kLFkc"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/mAP/outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DkjpIBARTQ7",
        "outputId": "36f9566e-ab68-4c3e-9d4f-8acaddb40385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/mAP\n",
            "Calculating mAP at 0.50 IoU threshold...\n",
            "100.00% = lego-car-mirror AP \n",
            "100.00% = lego-door-2 AP \n",
            "100.00% = lego-door-4 AP \n",
            "100.00% = lego-long-4 AP \n",
            "100.00% = lego-short-2 AP \n",
            "mAP = 100.00%\n",
            "Calculating mAP at 0.55 IoU threshold...\n",
            "100.00% = lego-car-mirror AP \n",
            "100.00% = lego-door-2 AP \n",
            "100.00% = lego-door-4 AP \n",
            "100.00% = lego-long-4 AP \n",
            "100.00% = lego-short-2 AP \n",
            "mAP = 100.00%\n",
            "Calculating mAP at 0.60 IoU threshold...\n",
            "100.00% = lego-car-mirror AP \n",
            "100.00% = lego-door-2 AP \n",
            "100.00% = lego-door-4 AP \n",
            "100.00% = lego-long-4 AP \n",
            "100.00% = lego-short-2 AP \n",
            "mAP = 100.00%\n",
            "Calculating mAP at 0.65 IoU threshold...\n",
            "100.00% = lego-car-mirror AP \n",
            "100.00% = lego-door-2 AP \n",
            "100.00% = lego-door-4 AP \n",
            "100.00% = lego-long-4 AP \n",
            "100.00% = lego-short-2 AP \n",
            "mAP = 100.00%\n",
            "Calculating mAP at 0.70 IoU threshold...\n",
            "75.00% = lego-car-mirror AP \n",
            "100.00% = lego-door-2 AP \n",
            "100.00% = lego-door-4 AP \n",
            "100.00% = lego-long-4 AP \n",
            "100.00% = lego-short-2 AP \n",
            "mAP = 95.00%\n",
            "Calculating mAP at 0.75 IoU threshold...\n",
            "75.00% = lego-car-mirror AP \n",
            "100.00% = lego-door-2 AP \n",
            "100.00% = lego-door-4 AP \n",
            "100.00% = lego-long-4 AP \n",
            "100.00% = lego-short-2 AP \n",
            "mAP = 95.00%\n",
            "Calculating mAP at 0.80 IoU threshold...\n",
            "75.00% = lego-car-mirror AP \n",
            "100.00% = lego-door-2 AP \n",
            "100.00% = lego-door-4 AP \n",
            "100.00% = lego-long-4 AP \n",
            "100.00% = lego-short-2 AP \n",
            "mAP = 95.00%\n",
            "Calculating mAP at 0.85 IoU threshold...\n",
            "75.00% = lego-car-mirror AP \n",
            "85.94% = lego-door-2 AP \n",
            "100.00% = lego-door-4 AP \n",
            "100.00% = lego-long-4 AP \n",
            "89.62% = lego-short-2 AP \n",
            "mAP = 90.11%\n",
            "Calculating mAP at 0.90 IoU threshold...\n",
            "75.00% = lego-car-mirror AP \n",
            "85.94% = lego-door-2 AP \n",
            "100.00% = lego-door-4 AP \n",
            "100.00% = lego-long-4 AP \n",
            "51.56% = lego-short-2 AP \n",
            "mAP = 82.50%\n",
            "Calculating mAP at 0.95 IoU threshold...\n",
            "50.00% = lego-car-mirror AP \n",
            "2.50% = lego-door-2 AP \n",
            "11.11% = lego-door-4 AP \n",
            "33.33% = lego-long-4 AP \n",
            "26.30% = lego-short-2 AP \n",
            "mAP = 24.65%\n",
            "\n",
            "***mAP Results***\n",
            "\n",
            "Class\t\tAverage mAP @ 0.5:0.95\n",
            "---------------------------------------\n",
            "lego-car-mirror\t\t82.50%\n",
            "lego-door-2\t\t87.44%\n",
            "lego-door-4\t\t91.11%\n",
            "lego-long-4\t\t93.33%\n",
            "lego-short-2\t\t86.75%\n",
            "\n",
            "Overall\t\t88.23%\n"
          ]
        }
      ],
      "source": [
        "%cd /content/mAP\n",
        "!python calculate_map_cartucho.py --labels=/content/labelmap.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9HPoOBVKvxU"
      },
      "source": [
        "The score reported at the end is your model's overall mAP score. Ideally, it should be above 50% (0.50). If it isn't, you can increase your model's accuracy by adding more images to your dataset. See my dataset video (*link to be added*) for tips on how to capture good training images and improve accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i40ve0SCLaE"
      },
      "source": [
        "# 7.&nbsp;Deploy TensorFlow Lite Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phT8vvzriqQp"
      },
      "source": [
        "Now that your custom model has been trained and converted to TFLite format, it's ready to be downloaded and deployed in an application! This section shows how to download the model and provides links to instructions for deploying it on the Raspberry Pi, your PC, or other edge devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq3L2IoP4VHp"
      },
      "source": [
        "## 7.1. Download TFLite model\n",
        "\n",
        "Run the two following cells to copy the labelmap files into the model folder, compress it into a zip folder, and then download it. The zip folder contains the `detect.tflite` model and `labelmap.txt` labelmap files that are needed to run the model in your application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awZMQGVqMpVL"
      },
      "outputs": [],
      "source": [
        "# Move labelmap and pipeline config files into TFLite model folder and zip it up\n",
        "!cp /content/labelmap.txt /content/drive/MyDrive/Colab_Notebooks/custom_model_lite\n",
        "!cp /content/labelmap.pbtxt /content/drive/MyDrive/Colab_Notebooks/custom_model_lite\n",
        "!cp /content/models/mymodel/pipeline_file.config /content/drive/MyDrive/Colab_Notebooks/custom_model_lite\n",
        "\n",
        "%cd /content/content/drive/MyDrive/Colab_Notebooks/custom_model_lite\n",
        "!zip -r ../custom_model_lite.zip *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "FVPfAGbNPV56",
        "outputId": "162879f3-8f3b-4583-8a08-f578a4db386e"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_7579b5f2-aa85-4d9e-a993-1b47be2b7ad2\", \"custom_model_lite.zip\", 634676)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/custom_model_lite.zip')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4VAvZo8qE4u5",
        "sxb8_h-QFErO",
        "eydREUsMGUUR",
        "eGEUZYAMEZ6f",
        "-19zML6oEO7l",
        "kPg8oMnQDYKl",
        "RDQrtQhvC3oG",
        "5i40ve0SCLaE",
        "WoptFnAhCSrR",
        "5VI_Gh5dCd7w"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}